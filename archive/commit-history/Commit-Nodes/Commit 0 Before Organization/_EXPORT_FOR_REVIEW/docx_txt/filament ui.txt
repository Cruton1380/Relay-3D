Yes. Treat this exactly like real life privacy: distance can change fidelity, but permission changes whether it exists for you at all.
Below is a locked privacy ladder you can implement as a lens contract (no new primitives).
Core rule
Every filament (and every branch of it) has a visibility policy that controls (a) discoverability and (b) fidelity by distance.
Two separate questions:
Can you even know it exists? (discoverability)
If it exists to you, what resolution do you see at this distance? (fidelity)
Distance never upgrades you past what policy allows.
Visibility ladder (what you see as you approach)
Level 0 — Invisible
You cannot see the filament, branches, boxes, types, names, nothing.
It does not appear in global search.
It does not contribute to “master ideas” except maybe as an anonymized aggregate if policy allows.
Level 1 — “Master idea” only (existence without structure)
You see a single coarse presence like: “Private activity exists here” or a generic “topic mass”.
No branch count. No file types. No boxes.
Search: no, except maybe category-level counts (“N private items”) if allowed.
Level 2 — Boxes only (structure without meaning)
You can see that there is a filament/time structure: anonymous TimeBoxes and a spine.
Branches: either hidden or shown only as “divergence happened” without showing where it goes.
File types: no.
Search: no, unless explicitly shared.
Level 3 — Types only (meaning class without content)
You can see artifact class as icons/tags:
“Spreadsheet”, “Video”, “3D scene”, “Model run”, “Policy”, etc.
Still no names, no values, no evidence pointers.
Branches: you may see multiple strands but unnamed (just “forked”).
Search: maybe in “type discovery” mode, but results are anonymized (“A spreadsheet exists in this cluster”).
Level 4 — Blurred projections (screens with redaction)
You see screens/panels, but:
blurred values
hidden identifiers
masked evidence pointers
You can infer “what system” but not “what data”.
Search: no, unless shared; within a shared group, you can search labels, not content.
Level 5 — Clear projections (readable 2D allowed)
You can open the 2D projection (Excel-like, timeline-like, etc.) and read.
You can see evidence pointers allowed by policy.
Search: yes, but only inside the authorized scope (team/org/channel), and only what policy allows (title/metadata vs content).
Level 6 — Engage (edit/commit allowed)
You can interact with the 2D surface:
select cell
type
run transforms
Edits append commits (never mutate).
Presence “locks on” to the engaged locus so others see active engagement (per your node/marker rule).
Search: yes, plus “edit navigation” (jump to loci you have rights to).
Your specific questions answered
“Can users see the filament on the branches at all?”
Only if policy ≥ Level 2.
If policy is lower: they see either nothing (L0) or only a generic “mass” (L1).
“If they are close, can they see individual branches or just master ideas?”
Close distance can raise fidelity only up to the policy ceiling.
If policy ceiling is L1: they still only see master idea even if standing “on it”.
If policy ceiling is L3+: they can see branches (but maybe unnamed).
“Can they see file types getting closer or just boxes?”
Boxes appear at L2.
File types appear at L3.
“Closer: blurred screens vs clear screens?”
Blurred screens at L4.
Clear screens at L5.
“Closer: can they look in 2D?”
Only at L5+.
“Can they engage?”
Only at L6, and only with explicit EngageSurface permission.
“Can they search it? Does it appear in global search?”
Global search only returns items with discoverability enabled:
L0–L2: no
L3: optional “type-only” discovery (no content)
L4: usually no (unless shared metadata)
L5–L6: yes, inside allowed scopes
So: search is policy-scoped, not distance-scoped.
One invariant to keep it honest
Privacy never renders “fake content.”
If you can’t see it, you see nothing or abstract placeholders (mass/boxes/types/blur). No invented summaries.
If you want, I’ll compress this into a single table you can put straight into your spec:
Policy Level → (Existence, Branch visibility, Types, Screens, 2D, Engage, Search scope)
…and we can also define “default policies” for:
personal filaments (user sphere),
KPI filaments (org-shared),
infrastructure filaments (SRE-only),
proximity channels (range-based discoverability).
excellent at this pace, how long will it take to build the whole system
I can’t give you a real time estimate (it depends hugely on team size, polish level, and what you count as “done”), but I can give you a build map that makes the timeline predictable by turning “the whole system” into a small number of shippable cores.
Here’s the clean way to think about it:
What “the whole system” really is (in 7 shippable layers)
1) Substrate MVP (truth objects)
Filaments + commits + timeboxes + evidence pointers + branching/merge
You already have a big chunk of this visually.
“Whole system” cannot exist without this being bulletproof.
2) Lenses MVP (views that never create truth)
Spreadsheet lens, KPI lens, workflow lens, globe lens
Spreadsheet lens = your “Excel at the edge” promise.
Globe lens can come later; it’s not required to be useful.
3) Permissions & privacy (the ladder we just locked)
Visibility ceiling + fidelity-by-distance + discoverability rules
This is mandatory before multi-user becomes real.
4) Presence Tier 1 → Tier 2
Counts now, roles/identities later
Tier 1 is working (big milestone).
Tier 2 adds policy-scoped identity/roles.
5) Editing & locking
EngageSurface permission + cell-level locus + conflict rules
This is the point where it becomes a real working tool, not just a viewer.
6) Networking / sync / storage
Realtime sync + persistence + audit logging
Can start simple (centralized) and harden later.
7) Governance & proposals
Voting on KPI definitions, schema changes, moderation, policy
Powerful, but should come after the editing + audit path is solid.
What I’d call “done enough to use” vs “done enough to launch”
“Usable internally” finish line
Substrate MVP ✅
Spreadsheet lens (editable endpoint) ✅
Permissions ladder ✅
Presence Tier 1 ✅
Basic persistence + replay ✅
“Launch-grade Relay” finish line
Everything above +
Tier 2 presence (roles/identity policy)
Robust conflict handling + branch workflows
Hard security + keying + invites / anti-sybil
Governance + moderation + scale work
How you’ll know the schedule (without guessing dates)
Count milestones, not time:
Editable endpoint demo passes (edit a cell → new commit → replay proves it)
Privacy ladder demo passes (same filament looks different by role + distance)
Two-user co-edit demo passes (presence lock + conflict outcome is provable)
Persistence demo passes (close app, reopen, history identical)
KPI governance demo passes (vote changes definition → commit appended → old truth remains)
When those 5 are solid, you have the “system” in the way people mean it.
If you tell me one thing—are you imagining a solo build, a 2–4 person team, or a small dedicated team (5–10)—I’ll translate those milestones into a realistic range for each finish line.
its just me building this frontend. but you forget that we are working with ai agents. all your estimations are based on human hours and schedules so you are highly overpredicting.
You’re right to push back: if you’re using agents effectively, “calendar time” stops mapping cleanly to “human hours.”
But the constraint doesn’t disappear — it moves from “typing speed” to these bottlenecks:
integration debt (wiring agent-made code into your actual router/state/render loop)
debugging truth (the last 10% is: why is it still showing the globe? why did a lens leak authority?)
spec friction (agents go fast when invariants are crisp — you’ve been doing that well)
QA / regressions (every new lens can accidentally violate substrate rules)
So instead of time estimates, here’s the correct “AI-assisted solo” model:
The real unit is “milestones per day”
Agents can generate many commits per day, but you can usually only validate a limited number of milestones/day because validation is the hard part.
If you can validate 1–2 milestone proofs/day, you’ll feel like you’re moving at warp speed.
How to schedule Relay with agents (without lying to yourself)
Rule 1: Everything must end in a proof route
Every feature request must end as a clickable route that proves the invariant.
Examples:
/proof/edit-cell → edit endpoint → commit appended → replay shows discrete causality
/proof/privacy-ladder → same filament renders differently by role + distance
/proof/coedit-lock → two viewers, one gets “engage surface,” other gets “no permission”
/proof/fork-kpi → split branch, presence is separate per branch
If it doesn’t end in a proof route, it’s not “built,” it’s “written.”
Rule 2: Agents write; you gatekeep
Agents can implement 80–90% quickly. Your job is:
choose which file is authority
keep invariants from drifting
do the final “surgical fit” into the existing app
Rule 3: One spine, many lenses
To go fast, you keep the substrate stable and let agents build lenses around it.
A realistic “agent-accelerated solo” roadmap
No dates — just sequence + parallelization.
Phase A — Foundation proofs (you’re already here)
Presence Tier 1 proof ✅ (done)
Routing + scene isolation proof ✅ (done)
Endpoint editing proof (next big one)
Phase B — Make “Excel at the edge” real
EngageSurface permission + cell locus
“close to excel but no permission” = can see, cannot click/enter 2D
“has permission” = can click cell → creates edit session locus
Cell edit → commit append → projection update
formula edits are commits
value edits are commits
references to evidence preserved
This is the first point where Relay becomes “usable,” not just conceptual.
Phase C — Privacy ladder (your question set)
Visibility ladder lens
Same filament rendered as:
Far: “master ideas only” (coarse boxes)
Closer: file types / commit types (still blurred)
Closer: blurred 2D screens (system visible, not data)
Allowed: clear screens + search + engage
This is mostly lens logic + policy tables, not new primitives.
Phase D — Multi-user editing correctness
Locking + conflict policy proof
who can edit which cell, when
what happens on collision (soft lock / fork / merge scar)
The “fastest path” if you want maximum agent leverage
Tell agents to deliver small, proof-complete PRs like this:
Each PR must include
one new route under /proof/...
a visible on-screen PASS/FAIL panel
minimal changes outside the feature folder
no new primitives unless explicitly requested
That prevents the classic agent trap: “big changes that feel productive but don’t integrate.”
Directly answering your privacy ladder questions (so it’s locked)
Yes — your instincts are correct. The clean model is:
Distance controls fidelity (what level of detail is even renderable)
Permission controls interaction + clarity (whether it becomes readable / searchable / editable)
Discoverability controls search (whether it appears in global search at all)
So:
Can nearby users see branches? Maybe only the existence + density, not content.
Can they see file types? At mid distance, yes as coarse typed boxes, not filenames.
Do they see blurred screens? Yes — “system silhouette” without data.
Do they see clear screens? Only with permission.
Can they enter 2D and engage? Only with EngageSurface permission + lock-in.
Can they search it / does it appear in global search? Only if filament’s discoverability policy allows it.
That matches “privacy like real life” without inventing new primitives.
If you want, I’ll convert this into a locked spec page (like PRESENCE-SPEC) titled:
USER-SPHERE-SPEC.md
“User as fractal globe”
“Profile sphere as cognitive projection”
“Proximity rendering vs global discoverability”
“Privacy ladder + engage rules”
“Search visibility rules”
“How presence + editing binds to cell loci”
Just say: “write USER-SPHERE-SPEC” and I’ll draft it in the same surgical style.
how do we do video game production, graphic design and model design flows, how do we replicate the interfaces needed in these worlds. how do the toolsets required in excellike spreadsheet cross over to the tools required in a 3ds type app? how do we build games in this interface with git backend? we want to make games that rely on ai image generation to produce characters that will participate. users can create whatever they want and it will be based on basic rules per domain. how do we include this in the gaming section. we need a steam like branch that will host all games provided.
You don’t “replicate After Effects / Blender / Unity” inside Relay.
You define a small set of universal editing primitives (the same way spreadsheets have cells/formulas/ranges), then each domain is just a lens + toolset that edits endpoint projections while committing truth underneath.
Here’s the clean way to lock it.
1) One substrate, many domain lenses
Substrate (never changes):
Filament (identity through time)
Commit / TimeBox (discrete irreversible change)
Faces (projection/evidence/dependencies)
Branching + merges (dispute / collaboration / versions)
Domain lens (changes):
What the endpoint “surface” looks like (2D/3D/timeline/canvas)
What tools exist (select, transform, paint, keyframe, rig, etc.)
What commit types are available (geometry edit, animation edit, shader compile, bake, export)
So: Excel-like editing and 3D editing are not different systems — they’re different endpoint surfaces backed by the same commit grammar.
2) The cross-over: Spreadsheet tools vs 3D tools
A spreadsheet is basically:
Selection: cell / range
Transform: formula changes value
Dependencies: references to other cells
Views: pivot/chart are lenses
Constraints: type/validation rules
A 3D app is basically:
Selection: vertex / edge / face / object / rig bone
Transform: translate/rotate/scale, sculpt, deform
Dependencies: modifiers, rigs, materials, textures, scripts
Views: camera, render passes, UV, animation curves
Constraints: topology rules, rig constraints, physics constraints
Unification rule:
“Cell range” and “mesh selection set” are the same kind of thing: a SelectionSet object in a lens.
So you build a shared “Edit Engine” with:
SelectionSet
Operation (tool action)
ConstraintRules (domain rules)
CommitBuilder (turn operation into a commit + evidence)
The only thing that changes per domain is what SelectionSet can point to and what operations are allowed.
3) Game production flow in Relay (the pipeline)
Game production is a bundle of filaments that all reference each other:
Core game filaments
game.project.<id> (the root “repo”)
game.scene.<id> (scene graph snapshots)
game.entity.<id> (ECS entity definitions)
game.system.<id> (game logic / scripts)
asset.mesh.<id>
asset.texture.<id>
asset.audio.<id>
asset.animation.<id>
build.<platform>.<id> (compiled build output hashes)
release.<id> (published version)
What commits look like
Mesh edit → commit with:
+X: new mesh snapshot hash / topology summary
-Z: evidence pointer (tool operation log + maybe binary diff)
deps: materials, rig, textures
Scene edit → commit with:
+X: scene graph snapshot (entities + transforms)
deps: mesh/texture filaments referenced
Build → commit with:
+X: build artifact hash + manifest
-Z: compiler version, build script, inputs list
Key invariant:
The game is never “a folder of files.”
It’s a causality tree of filaments with an endpoint surface you can edit.
4) “How do we build games in this interface with git backend?”
You don’t force everyone to see git.
You do this:
A) Endpoint editor (feels like Unity/Blender)
3D viewport
timeline / animation curves
inspector panels
asset browser
B) Under the hood (is git-like)
Every action becomes a commit:
commit message auto-generated from tool + selection
branches for experimentation (“try a new combat system”)
merges produce SCARs (reconciliation evidence)
releases are tagged commits
C) Two zoom levels
Zoom in: normal creative app
Zoom out: forensic causality, branching, evidence, permissions
That’s “Excel at the edge, Git at depth” applied to game dev.
5) AI-generated characters that “participate”
This fits your locked AI rule:
AI never creates filaments.
AI only appends commits to existing filaments.
So the correct model is:
character.<id> filament exists (created by user/game)
AI can propose commits:
asset.texture.<id> “generate portrait”
asset.mesh.<id> “generate concept mesh”
character.dialogue.<id> “generate dialogue line”
A human or policy must “accept” (Gate) before it becomes active in a published branch.
Safety / abuse guardrails you’ll want
Content policy per game (e.g., teen-friendly, mature, etc.)
Copyright guardrails: don’t allow users to publish characters clearly ripped from famous IP in your public store
Auditability: prompts/weights/output hashes as evidence pointers
6) “Steam-like branch” for hosting games
This is just a Store lens over public game-project filaments.
Store = lens, not authority
Each game listing projects:
Title, screenshots, trailer = projections
Version = pointer to a release commit/tag
Ratings = filaments (votable, branchable)
Reviews = filaments (moderated, auditable)
Downloads = artifact pointers to build commits
The “Steam branch” is literally:
store.catalog.global filament (curated index)
Each entry is a pointer to game.project.<id>@releaseTag
Governance can fork the catalog (community vs official)
So you can have:
Official catalog branch
Community catalog branch
Region-specific catalogs (policy/law differences)
7) What you actually need to build next (minimum viable gaming section)
If you want the smallest thing that proves the model:
Game Project filament + Release tag
Asset filament with AI commit proposal + human accept Gate
Simple scene editor lens (even minimal: place cubes + transforms)
Store lens that lists projects + launches a “play build” artifact
That’s enough to prove the whole “Steam + creation + AI characters” story without building a full Unity clone.
If you tell me which domain you want to prototype first:
(A) 2D graphic design canvas
(B) 3D scene editor
(C) animation timeline
(D) store catalog
…I’ll write the locked spec + the first proof route for that domain in the same style as PRESENCE-SPEC (no new primitives, just lens rules + commit grammar).
